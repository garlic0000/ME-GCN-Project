import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import os
import numpy as np

"""
å…³é”®æ›´æ”¹ï¼š
åœ¨model_2çš„åŸºç¡€ä¸Š
é€šé“æ•°ä¸å˜ï¼š

å°† GraphAttentionLayer æ›¿æ¢ä¸º SelfAttentionLayerã€‚
è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸å†ä¾èµ–é‚»æ¥çŸ©é˜µï¼Œè€Œæ˜¯æ ¹æ®ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§è¿›è¡Œè®¡ç®—ã€‚
è®¡ç®—çš„æ¯ä¸€æ­¥ä¿æŒä¸åŸæ¨¡å‹ç›¸ä¼¼çš„ç»“æ„å’Œè¡Œä¸ºï¼Œåªæ˜¯ç”¨è‡ªæ³¨æ„åŠ›ä»£æ›¿äº†å›¾æ³¨æ„åŠ›ã€‚

è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼š

ä½¿ç”¨ Query (Q)ã€Key (K) å’Œ Value (V) æ¥è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œå…¬å¼å¦‚ä¸‹
è¿™é‡Œï¼ŒQã€ğ¾å’Œ ğ‘‰é€šè¿‡è¾“å…¥ç‰¹å¾è®¡ç®—å¾—å‡ºï¼Œå¹¶ä¸”è¿›è¡Œç¼©æ”¾å’Œå½’ä¸€åŒ–ã€‚
æ— é‚»æ¥çŸ©é˜µï¼š

è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›´æ¥è®¡ç®—è¾“å…¥ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œä¸å†ä¾èµ–å¤–éƒ¨çš„é‚»æ¥çŸ©é˜µã€‚
ä¸å†ä¾èµ–å›¾ç»“æ„ï¼š

ç”±äºè‡ªæ³¨æ„åŠ›æ˜¯åŸºäºç‰¹å¾ä¹‹é—´çš„å…³ç³»è€Œéå›¾ç»“æ„è¿›è¡Œçš„ï¼Œå› æ­¤ä¸éœ€è¦æ˜¾å¼åœ°ä¼ é€’é‚»æ¥çŸ©é˜µã€‚
"""

class GraphConvolution(nn.Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    Param:
        in_features, out_features, bias
    Input:
        features: N x C (n = # nodes), C = in_features
        adj: adjacency matrix (N x N)
    """

    def __init__(self, in_features, out_features, mat_path, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(in_features, out_features))  # no weight_norm
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

        adj_mat = np.load(mat_path)
        self.register_buffer('adj', torch.from_numpy(adj_mat))

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        b, n, c = input.shape
        # Apply weight to the input
        support = torch.bmm(input, self.weight.unsqueeze(0).repeat(b, 1, 1))  # Shape: [B, N, F] x [F, O]
        output = torch.bmm(self.adj.unsqueeze(0).repeat(b, 1, 1), support)  # Shape: [B, N, N] x [B, N, O]

        if self.bias is not None:
            return output + self.bias  # Shape: [B, N, O]
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
            + str(self.in_features) + ' -> ' \
            + str(self.out_features) + ')'


class GCN(nn.Module):
    def __init__(self, nfeat, nhid, nout, mat_path, dropout=0.3):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat, nhid, mat_path)
        self.bn1 = nn.BatchNorm1d(nhid)

    def forward(self, x):
        x = self.gc1(x)
        x = x.transpose(1, 2).contiguous()
        x = self.bn1(x).transpose(1, 2).contiguous()
        x = F.relu(x)

        return x, self.gc1.adj  # è¿”å›å›¾çš„é‚»æ¥çŸ©é˜µ adj


class SelfAttentionLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super(SelfAttentionLayer, self).__init__()

        # è¾“å…¥è¾“å‡ºé€šé“æ•°ç›¸åŒï¼Œä¿è¯ä¸æ”¹å˜é€šé“æ•°
        assert in_features == out_features, "in_features and out_features should be the same to preserve channel size"

        # æƒé‡çŸ©é˜µ Q, K, V
        self.W_q = nn.Parameter(torch.Tensor(in_features, out_features))  # Query
        self.W_k = nn.Parameter(torch.Tensor(in_features, out_features))  # Key
        self.W_v = nn.Parameter(torch.Tensor(in_features, out_features))  # Value
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.W_q.size(1))
        self.W_q.data.uniform_(-stdv, stdv)
        self.W_k.data.uniform_(-stdv, stdv)
        self.W_v.data.uniform_(-stdv, stdv)

    def forward(self, h):
        # è¾“å…¥ h çš„å½¢çŠ¶: [B, N, F]

        # è®¡ç®— Q, K, V
        Q = torch.matmul(h, self.W_q)  # Shape: [B, N, F]
        K = torch.matmul(h, self.W_k)  # Shape: [B, N, F]
        V = torch.matmul(h, self.W_v)  # Shape: [B, N, F]

        # è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯æ³¨æ„åŠ›å¾—åˆ†
        attention_scores = torch.matmul(Q, K.transpose(1, 2))  # Shape: [B, N, N]

        # ç¼©æ”¾
        attention_scores = attention_scores / math.sqrt(K.size(-1))

        # ä½¿ç”¨ softmax è®¡ç®—å½’ä¸€åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°
        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)  # Shape: [B, N, N]

        # ä½¿ç”¨æ³¨æ„åŠ›åˆ†æ•°åŠ æƒæ±‚å’Œ V
        h_prime = torch.matmul(attention_weights, V)  # Shape: [B, N, F]

        return h_prime


class AUwGCN(torch.nn.Module):
    def __init__(self, opt):
        super().__init__()
        # æœåŠ¡å™¨æµ‹è¯•è·¯å¾„
        mat_dir = '/kaggle/working/ME-GCN-Project'
        mat_path = os.path.join(mat_dir, 'assets', '{}.npy'.format(opt['dataset']))

        self.graph_embedding = torch.nn.Sequential(GCN(2, 16, 16, mat_path))

        in_dim = 192  # ä¿æŒè¾“å…¥é€šé“æ•°ä¸º192

        self.attention = SelfAttentionLayer(in_features=16, out_features=16)  # ä½¿ç”¨è‡ªæ³¨æ„åŠ›å±‚

        self._sequential = torch.nn.Sequential(
            torch.nn.Conv1d(in_dim, 64, kernel_size=1, stride=1, padding=0, bias=False),
            torch.nn.BatchNorm1d(64),
            torch.nn.ReLU(inplace=True),

            torch.nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),
            torch.nn.BatchNorm1d(64),
            torch.nn.ReLU(inplace=True),

            torch.nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=2, dilation=2, bias=False),
            torch.nn.BatchNorm1d(64),
            torch.nn.ReLU(inplace=True),
        )

        self._classification = torch.nn.Conv1d(64, 3 + 3 + 2 + 2, kernel_size=3, stride=1, padding=2, dilation=2,
                                               bias=False)

        self._init_weight()

    def forward(self, x):
        b, t, n, c = x.shape

        x = x.reshape(b * t, n, c)  # (b*t, n, c)
        x, adj = self.graph_embedding(x)  # è·å–å›¾å·ç§¯çš„è¾“å‡º
        x = self.attention(x)  # ä¼ å…¥è‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œå¤„ç†

        x = x.reshape(b, t, -1).transpose(1, 2)  # è°ƒæ•´ç»´åº¦
        x = self._sequential(x)
        x = self._classification(x)

        return x

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, torch.nn.Conv1d):
                torch.nn.init.kaiming_normal_(m.weight)
            if isinstance(m, torch.nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)



